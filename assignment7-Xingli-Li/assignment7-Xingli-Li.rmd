---
title: "CS 422 Homework 7"
author: "Xingli Li"
output:
  html_notebook: 
    toc: yes
    toc_float: yes
    toc_depth: 5
  html_document:
    toc: yes
    df_print: paged
---

## Homework 7

### Part 2

set work space and install dependent package.

```{r}
# install.packages("keras")
library(keras)
library(dplyr)
library(caret)
setwd("/media/eric/Data/IIT/CS422/cs422_hw/assignment7-Xingli-Li")
```

### 2.1-(a)

Loading origin data and processing data.

```{r}
df <- read.csv("activity-small.csv")

set.seed(1122)
df <- df[sample(nrow(df)), ] # Shuffle, as all of the data in the .csv file
                             # is ordered by label!  This will cause problems
                             # if we do not shuffle as the validation split
                             # may not include observations of class 3 (the
                             # class that occurs at the end).  The validation_
                             # split parameter samples from the end of the
                             # training set.


indx <- sample(1:nrow(df), 0.20*nrow(df))
test.df  <- df[indx, ]
train.df <- df[-indx, ]

label.test <- test.df$label
test.df$label <- NULL
test.df <- as.data.frame(scale(test.df))
test.df$label <- label.test
rm(label.test)

label.train <- train.df$label
train.df$label <- NULL
train.df <- as.data.frame(scale(train.df))
train.df$label <- label.train
rm(label.train)
rm(indx)

```

```{r}
use_condaenv("base")

create_model <- function (){
  model <- keras_model_sequential()  # input layer
  layer_flatten(model,input_shape = c(3) )
  layer_dense(model,units = 128, activation = 'relu')  # hidden layer # dropout
  layer_dense(model, units = 4, activation = 'softmax')     # output layer

  compile(model,
          optimizer = optimizer_adam(),
          loss = 'categorical_crossentropy',
          metrics = 'accuracy')
  model
}

model <- create_model()
summary(model)

```

training

```{r}

train_X <- as.matrix(train.df[,1:3])
train_y <- to_categorical(train.df$label,4, dtype = "int32")

history <- model %>% fit(train_X,
                         train_y,
                         epochs = 100,
                         batch_size = 1,
                         validation_split = 0.2)
plot(history)
```

evaluate model

```{r}
options(digits = 3)
test_X <- as.matrix(test.df[,1:3])
test_y <- to_categorical(test.df$label,4, dtype = "int32")
model %>% evaluate(test_X, test_y)
```

confusing matrix

```{r}
pred <- model %>% predict(test_X)
pred_res <-  apply(pred,1,which.max)-1
test_res <-  test.df$label
table(pred_res,test_res)

```

### 2.1-(a)-(i)

What is the overall accuracy of your model on the test dataset?

Answer:
      The overall accuracy of the model: 0.815

### 2.1-(a)-(ii)

What is the per-class sensitivity, specificity, and balanced accuracy on the test dataset?

```{r}

get_metrics <- function (pred_res,test_res){

  xtable <- confusionMatrix(table(pred_res,test_res))

  # Extract acc, bal.acc, error, spec, sens, and prec from the variable a.
  acc <- xtable$overall["Accuracy"]
  bal.acc <- xtable$byClass[,"Balanced Accuracy"]
  spec <- xtable$byClass[,"Specificity"]
  sens <- xtable$byClass[,"Sensitivity"]
  prec <- xtable$byClass[,"Precision"]

  results.df <- data.frame(sens = sens,
                           spec = spec,
                           bal.acc = bal.acc,
                           prec = prec)

  print("Batch gradient descent")
  print(paste0("Overall accuarcy:",acc))

  class_range <- c(0,1,2,3)

  for (i in class_range){
    class <- paste0("Class: ",i)
    print(paste(class,
                ": Sens. =",round(results.df$sens[i+1], 3),
                ", Spec. =",round(results.df$spec[i+1],3),
                ", Bal.Acc. =",round(results.df$bal.acc[i+1],3)
    ))
  }

  c(acc,results.df)
}

metrics <- get_metrics(pred_res,test_res)

```


### 2.1-(b)

```{r}

print_metrics2 <- function (batch_size, time, pred_res, test_res){

  xtable <- confusionMatrix(table(pred_res,test_res))

  acc <- xtable$overall["Accuracy"]
  bal.acc <- xtable$byClass[,"Balanced Accuracy"]
  spec <- xtable$byClass[,"Specificity"]
  sens <- xtable$byClass[,"Sensitivity"]
  prec <- xtable$byClass[,"Precision"]

  results.df <- data.frame(sens = sens,
                           spec = spec,
                           bal.acc = bal.acc,
                           prec = prec)

  print(paste("Batch size:",batch_size))
  print(paste0("Time taken to train neural network: ",round(time,3)," (seconds)"))
  print(paste0("Overall accuarcy: ",acc))

  class_range <- c(0,1,2,3)
  for (i in class_range){
    class <- paste0("Class: ",i)

    print(paste(class,
                ": Sens. =",round(results.df$sens[i+1], 3),
                ", Spec. =",round(results.df$spec[i+1],3),
                ", Bal.Acc. =",round(results.df$bal.acc[i+1],3)
    ))
  }
  print("")

  c(acc,results.df)
}


options(digits = 3)

batch_size_range <- c(1,32,64,128,256)

for (batch_size in batch_size_range){
  begin <- Sys.time()

  model <- NULL
  model <- create_model()

  history <- model %>% fit(train_X,
                           train_y,
                           epochs = 100,
                           batch_size = batch_size,
                           validation_split = 0.2)
  end <- Sys.time()
  time <- end-begin

  pred <- model %>% predict(test_X)
  pred_res <-  apply(pred,1,which.max)-1

  print_metrics2(batch_size, time, pred_res, test_res)

}


```

### 2.1-(c)-(i)

Analyze the output from the mini-batch gradient descent. Why do you think that the time vary as you increase the batch size?

### 2.1-(c)-(ii)

Comment on the output from the mini-batch gradient descent. Does overall accuracy, balanced accuracy and per-class statistics remain the same? Change? If change, why?

### 2.1-(d)-(i)

Overall accuracy of your model on the test dataset.

### 2.1-(d)-(ii)

Per-class sensitivity, specificity, and balanced accuracy on the test dataset. Pick the construction that had the best performance results and compare that to the performance you observed in (a). Comment on the changes you observed by adding a new hidden layer. (Does the performance increase? Decrease? Stay the same?)
