---
title: "CS 422 Homework 4"
author: "Xingli Li"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    toc: yes
    df_print: paged
---

## Homework 4

set work space and install dependent package.

```{r}

setwd(getwd())

#install.packages(rpart)
library(rpart)  #classification and regression trees
library(rpart.plot)
#install.packages(caret)
library(caret)
#install.packages(partykit)
library(partykit) #treeplots
#install.packages(ROCR)
library(ROCR)
```

### Part 2.1-a

```{r}

df.test <- read.csv(file = "/media/eric/Data/IIT/CS422/cs422_hw/assignment4-Xingli-Li/adult-test.csv", encoding = "UTF-8")
df.train <- read.csv(file = "/media/eric/Data/IIT/CS422/cs422_hw/assignment4-Xingli-Li/adult-train.csv", encoding = "UTF-8")

NROW(df.test)
NROW(df.train)
typeof(df.test)
col.names <- names(df.test)

print(col.names)

for (col in col.names) {
  if (sum(df.test[col] == "?") > 0) {
    # The column contain '?' in test data set.
    print(paste("the column '", col, "' in test data"))
  }
  if (sum(df.train[col] == "?") > 0) {
    # The column contain '?' in train data set.
    print(paste("the column '", col, "' in train data"))
  }
}

df.test.cleaned <- df.test[-which(df.test['workclass'] == '?'
                                   |
                                   df.test['occupation'] == '?'
                                   |
                                   df.test['native_country'] == '?'),]
print(paste("test data set total:",NROW(df.test.cleaned)))

df.train.cleaned <- df.train[-which(df.train['workclass'] == '?'
                                   |
                                   df.train['occupation'] == '?'
                                   |
                                   df.train['native_country'] == '?'),]

print(paste("train data set total:", NROW(df.train.cleaned)))


# string to number
# df.test.cleaned <- data.frame(df.test.cleaned[,1:14],
#                              income=ifelse(df.test.cleaned$income=="<=50K",0, 1))
#
# df.train.cleaned <- data.frame(df.train.cleaned[,1:14],
#                               income=ifelse(df.train.cleaned$income=="<=50K",0, 1))

```

### Part 2.1-b-i

```{r}

my_tree <- rpart(income ~ ., data = df.train.cleaned, method="class")

summary(my_tree)

plotcp(my_tree)

# plot method of the partykit package.
plot(as.party(my_tree))

# Drawing decision classification tree
rpart.plot(my_tree, extra=104, fallen.leaves=T, type=4,
           main="Decision Tree") 

```

    capital_gain;
    education;
    relationship

### Part 2.1-b-ii

    The first split is done on "relationship" predictor.

    the predicted class of the first node is "<=50k".

    the distribution of observations at first node is:
    observations: <=50K   >50K
    class counts: 22653   7508
    probabilities: 0.751  0.249

### Part 2.1-c-i

```{r}

X.test <- df.test.cleaned[,1:14]
y.test <- df.test.cleaned$income

# predict the test dataset
y_hat <- predict(my_tree, newdata = X.test,type = "class")

confuseMatrix <-table(y_hat,y.test)
print(confuseMatrix)

# confusion matrix function
confx <-confusionMatrix(y_hat, as.factor(y.test))


sprintf(paste("recall: ",round(recall(confuseMatrix), digit = 3)))


print(paste("precision:",round(precision(confuseMatrix), digit = 3)))


print(paste("F1 value:",round(F_meas(confuseMatrix), digit = 3)))


balanced_accuracy = mean( c(sensitivity(confuseMatrix),specificity(confuseMatrix)))

print(paste("balanced_accuracy:",round(balanced_accuracy, digit = 3) ))
```

### Part 2.1-c-ii

```{r}

# Balanced error rate = 1.0 – balanced accuracy
balanced_error_rate <- 1.0 - balanced_accuracy
print(paste("balanced error rate:",round(balanced_error_rate,digits = 3)))
```

### Part 2.1-c-iii

```{r}
# sensitivity ,specificity

# sensitivity: The true positive rate is the sensitivity

# specificity: The false positive rate is 1-specificity

print(paste("sensitivity:",round(sensitivity(confuseMatrix), digit = 3)))

print(paste("specificity:",round(specificity(confuseMatrix), digit = 3)))

```

### Part 2.1-c-iv

```{r}
pred.rocr <- predict(my_tree, newdata = X.test, type = "prob")[,2]

f.pred <- prediction(pred.rocr, y.test)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
```

```{r}

auc <- performance(f.pred, measure = "auc")

print(paste("The AUC of the ROC curve is:",round(auc@y.values[[1]],digits = 3)))
```

### Part 2.1-d

```{r}
## printcp() will tell you how much CP value is divided into each layer and what the average relative error is.
printcp(my_tree)

plotcp(my_tree)

cp <- my_tree$cptable[which.min(my_tree$cptable[,"xerror"]), "CP"]

```

```{r}

# cp: complexity parameter of the node.
# nsplit: Number of tree splits.
# rel error: Relative error = xerror ± xstd.
# xerror: Estimation error of cross validation.
# xstd: Standard deviation of cross validation process.

print(paste("The CP is complexity parameter of the node, the CP of the model",cp))
```

Pruning:

```{r }

model.pruned <- prune(my_tree, cp=cp)

# Redrawn
rpart.plot(model.pruned, extra=104, fallen.leaves=T, type=4, main="Decision Tree pruned")

```

```{r}

pruned.pred <- predict(model.pruned, X.test, type="class")

confusionMatrix(pruned.pred, as.factor(y.test))

confusionMatrix(y_hat, as.factor(y.test))

#compare_models(my_tree, model.pruned)
```

From comparing, the effect is not improved.

### Part 2.1-e-i

```{r}

count.smaller <- sum(df.train.cleaned$income=="<=50K")
count.bigger <- sum(df.train.cleaned$income==">50K")

print("In the training dataset,")
print(paste("th observations of the class “<=50K” are:",count.smaller))
print(paste("th observations of the class “>50K” are:",count.bigger))
```

### Part 2.1-e-ii

```{r}
set.seed(1122)

df.train.smaller <- df.train.cleaned[df.train.cleaned$income=="<=50K",]
df.train.bigger <- df.train.cleaned[df.train.cleaned$income ==">50K",]

new.df.train.smaller <- df.train.smaller[sample(nrow(df.train.smaller),
                                               count.bigger,replace = FALSE),]

new.df.train <- rbind(new.df.train.smaller, df.train.bigger)

```

### Part 2.1-e-iii-i

```{r}

# Training
my_tree_new <- rpart(income ~ ., data = new.df.train, method="class")

summary(my_tree_new)

printcp(my_tree_new)

plotcp(my_tree_new)

```

```{r}

# predict the test dataset
y_hat_new <- predict(my_tree_new, newdata = X.test,type = "class")

# confusion matrix function
confusMatrix_new <-confusionMatrix(y_hat_new, as.factor(y.test))
print(confusMatrix_new)

# Drawing decision classification tree
rpart.plot(my_tree_new, extra=104, fallen.leaves=T, type=4, main="Decision Tree")

```

```{r}

print(paste("Balanced Accuracy of the new model in the test data set is:",
            round(confusMatrix_new$byClass["Balanced Accuracy"],digits = 3)))
```

### Part 2.1-e-iii-ii

```{r}
# balanced_error_rate
print(paste("Balanced error rate of the new model in the test data set is:",
           round(1.0 - confusMatrix_new$byClass["Balanced Accuracy"],digits = 3)))
```

### Part 2.1-e-iii-iii

```{r}
# sensitivity,Specificity

print(paste("Sensitivity of the new model in the test data set is:",
           round(confusMatrix_new$byClass["Sensitivity"],digits = 3)))

print(paste("Specificity of the new model in the test data set is:",
           round(confusMatrix_new$byClass["Specificity"],digits = 3)))
```

### Part 2.1-e-iii-iv

```{r}
# ROC curve, what is the AUC?

pred.rocr <- predict(my_tree_new, newdata = X.test, type = "prob")[,2]

f.pred <- prediction(pred.rocr, y.test)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
```

```{r}

auc_new <- performance(f.pred, measure = "auc")


print(paste("The AUC of the ROC curve is:",round(auc_new@y.values[[1]],digits = 3)))
```

### Part 2.1-f

```{r}

print(paste("The test data size of the class '<=50K' is:",NROW(df.test.cleaned[df.test.cleaned$income=="<=50K",])))
print(paste("The test data size of the class '>50K' is:",NROW(df.test.cleaned[df.test.cleaned$income==">50K",])))


confusionMatrix(y_hat, as.factor(y.test))

confusionMatrix(y_hat_new, as.factor(y.test))

print(paste("the auc of the before treatment is:",round(auc@y.values[[1]],digits = 3)))

print(paste("the auc of the after treatment is:",round(auc_new@y.values[[1]],digits = 3)))
```

Balanced Accuracy is increased,

Sensitivity is decrease,

Specificity is increased,

Positive predictive value is increased,

AUC is increased a little.

Sensitivity is true positive rate, which means a model correctly predicts the proportion of positive samples.

Because positive samples are too more than negitive samples , it was overestimated before treatment, current Sensitivity is reasonable value.
