---
title: "CS 422 Homework 5"
author: "Xingli Li"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    toc: yes
    df_print: paged
---

## Homework 5

### Part 2

set work space and install dependent package.

```{r}

library(rpart)  #classification and regression trees
library(rpart.plot)
#install.packages("caret")
#install.packages("randomForest")
library(randomForest)
library(caret)
library(ROCR)
work.path <- getwd()
setwd(work.path)
```

### 2.1-(1)

Loading data and cleaning data

```{r}
df.test <- read.csv(file = "./adult-test.csv", encoding = "UTF-8")
df.train <- read.csv(file = "./adult-test.csv", encoding = "UTF-8")

col.names <- names(df.test)

df.test <- df.test[-which(df.test['workclass'] == '?'
                            |
                            df.test['occupation'] == '?'
                            |
                            df.test['native_country'] == '?'),]


df.train <- df.train[-which(df.train['workclass'] == '?'
                              |
                              df.train['occupation'] == '?'
                              |
                              df.train['native_country'] == '?'),]


# df.test$income <- factor(df.test$income)

# df.train$income <- factor(df.train$income)

```

Init variable.

```{r}

set.seed(1122)

n <- NROW(col.names) - 1

mtree.range <- c(250, 500, 750)

mtry.range <- c(sqrt(n), sqrt(n) + 1, sqrt(n) + 2)

```

Definition function

```{r}

learn_model <- function(train.df, ntree.param, mtry.param) {

  train.df$income <- as.factor(train.df$income)
  model <- randomForest(income ~ ., method = "class",
                        ntree = ntree.param,
                        mtry = mtry.param,
                        data = train.df)
  model
}


get_results <- function(model, pred, test.df) {

  a <- confusionMatrix(pred, as.factor(test.df$income))

  # Extract num.tree and mtry.param from the model object
  num.tree <- model$ntree
  mtry.param <- model$mtry

  # Extract acc, bal.acc, error, spec, sens, and prec from the variable a.
  acc <- a$overall["Accuracy"]
  bal.acc <- a$byClass["Balanced Accuracy"]
  error <- 1.0 - acc
  spec <- a$byClass["Specificity"]
  sens <- a$byClass["Sensitivity"]
  prec <- a$byClass["Precision"]

  # Extract oob from model$err.rate
  oob <- model$err.rate[1]

  results.df <- data.frame(ntree = num.tree,
                           mtry = mtry.param,
                           acc = acc,
                           bal.acc = bal.acc,
                           error = error,
                           spec = spec,
                           sens = sens,
                           prec = prec,
                           oob = oob)
  rm(a)
  results.df
}
```

### 2.1-(2)-(a)

Iteration of the grid search.

```{r}

results.df <- data.frame(ntree = 0,
                         mtry = 0,
                         acc = 0,
                         bal.acc = 0,
                         error = 0,
                         spec = 0,
                         sens = 0,
                         prec = 0,
                         oob = 0) # Prime the results data frame.


for (ntree in mtree.range) {
  for (mtry in mtry.range) {
    str <- paste(Sys.time(), ": Starting model training on", ntree, "trees, and",
                 floor(mtry), "attributes.\n")
    cat(str)
    model <- NULL
    model <- learn_model(df.train, ntree, mtry)
    str <- paste(Sys.time(), ": Done.\n")
    cat(str)
    pred <- predict(model, newdata = df.test, type = "class")
    tmp.df <- get_results(model, pred, df.test)
    results.df <- rbind(results.df, tmp.df)
    rm(tmp.df)
  }
}

rownames(results.df) <- c()
results.df <- results.df[-1, ] # Remove the first row (used to prime the frame)
round(results.df, 3)
```

Determine the best model by examining the maximum balanced accuracy, sensitivity and specificity.

```{r}
results.df <-results.df[order(results.df$bal.acc,
                              results.df$sens,
                              results.df$spec,
                              decreasing=TRUE),]

result  <- results.df[1,]

print(paste("Grid search resulted in the best model at ntree =",
            round(result$ntree,digits = 3),
            " and mtry =", round(result$mtry,digits = 3)))
print(paste("Accuracy =",round(result$acc,digits = 3)))
print(paste("Balanced Accuracy = ",round(result$bal.acc,digits = 3)))
print(paste("Sensitivity =",round(result$sens,digits = 3)))
print(paste("Specificity =",round(result$spec,digits = 3)))
```

### 2.1-(2)-(b)

Determine the best model by examining the lowest (minimum) OOB error rate.

```{r}
result <-results.df[which.min(results.df$oob),]

print(paste("Grid search resulted in the best model for OOB at ntree =",
            round(result$ntree,digits = 3),
            " and mtry =", round(result$mtry,digits = 3)))
print(paste("OOB =",round(result$oob,digits = 3)))
```

### 2.1-(2)-(c)

Is the best model as determined by (a) the same model as determined by (b). Justify your answer.

       (a) is the best model. 
       Because (a) ntree is smaller, mtry just is a little larger. this model is simple and efficient,it is less generalization error.
