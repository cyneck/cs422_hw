---
title: "CS 422 Homework 9"
author: "Xingli Li"
output:
  html_notebook: 
    toc: yes
    toc_float: yes
    toc_depth: 5
  html_document:
    toc: yes
    df_print: paged
editor_oprions:
  markdown:
    wrap: 100
---

## Homework 9

### Part 2

set work space and install dependent package.

```{r}
library(cluster)
library(ggplot2)
library(factoextra)
library(fpc)
library(dbscan)

setwd(getwd())
```

### 2.1-(a)

Do you think it is necessary to standardize the dataset? Justify your answer.

Answer:

      I think it is necessary to standardize the dataset. From the figure below, although we can see that there is no obvious change before and after standardization, standardization of data is the scaling of data to the same data interval and range to reduce the impact of scale, characteristics, distribution differences, etc. on the model. In addition to model calculation, the standardized data also has the meaning of directly calculating and generating composite indicators, which is a necessary step for weighting indicators.

```{r}

df <- read.csv("s1.csv", encoding = 'UTF-8')
df.standardized <- scale(df)

plot(df, main="Raw points")

plot(df.standardized, main="Standardized points")
```

### 2.1-(b)

Plot the dataset and describe in 1-2 sentences what you observe (visually) in the plot: how many clusters do you see? Are they well-separated?

Answer:

      There are 15 clusters in the plot. They are all clustered based on density and  they are well-separated, some are circular, rectangular, and some are long strips.

```{r}
plot(df.standardized, main="The dataset points")
```

### 2.1-(c)-(i)

Let's see how many clusters K-Means finds.

Using the "wss" method, draw the scree plot for the optimal number of clusters

```{r}

# set.seed(123)

# indx <- sample(1:nrow(df), 0.20*nrow(df))

# df_sample <- df[indx,]

fviz_nbclust(df.standardized, kmeans, method="wss",k.max=20)
```

### 2.1-(c)-(ii)

Using the "silhouette" method, draw the scree plot for the optimal number of clusters.

```{r}

fviz_nbclust(df.standardized, kmeans, method="silhouette", k.max=20)
```

### 2.1-(c)-(iii)

What do you think is the appropriate number of clusters if we were to use K-Means clustering on this dataset?

Answer:

      I think the appropriate number of clusters is 16.

### 2.1-(d)-(i)

Using the answer to (c)(iii), perform K-Means clustering on the dataset and plot the results.

```{r}
km <- kmeans(df.standardized, centers=16,nstart=16)

fviz_cluster(km, df.standardized)
```

### 2.1-(d)-(ii)

Comment on how K-Means has clustered the dataset. (1-2 sentences.)

Answer:

      The K-Means algorithm does not cluster well in this dataset. There are clusters were merged, clearly different from what we observed.

### 2.1-(e)-(i)

We will now perform dbscan on this dataset.

What is the value of MinPts that you think is reasonable for this dataset? Why?

Answer:

      I think that the value of MinPts is 4 for this dataset. 
      minPts: The minimum number of points (a threshold) clustered together for a region to be considered dense. 
      According to Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited. For two-dimensional data: use default value of minPts=4 (Ester et al., 1996). For more than 2 dimensions: minPts=2*dim (Sander et al., 1998). A low minPts means it will build more clusters from noise, so don't choose it too small. larger minPts values are usually better for data sets with noise. As a rule of thumb, minPts = 2·dim can be used.

### 2.1-(e)-(ii)

In order to find the value of ɛ (eps), we need to calculate the average distance of every point to its k nearest neighbors. Set the value of k to be the result you obtained in (e)(i). Then, using this value determine what the correct value for ɛ should be. (Hint: Look at the online manual page for the function kNNdistplot()).

```{r}
# find the best value of ɛ that clusters by kNNdistplot()
# Find the 'elbow' in the graph.

k <- 4
eps <- 0.087

dbscan::kNNdistplot(df.standardized, k)
abline(h = eps, col = "red",lty=2)
```

```{r}
dbscan(df.standardized, eps, minPts = k)
```

Answer:

      At minPts = 4, eps = 0.087, there are 15 clusters.

### 2.2-(a)

The dataset is available in Blackboard in the file countries.csv. Read in the dataset in a data frame taking care to allocate the first column to the name of the row instead of an attribute; this will allow the row name to be displayed in a PCA biplot. (Hint: See row.names parameter for the read.csv(...) method.)

### 2.2-(a)-(i)

Print a summary of all of the attributes in the dataset to become familiar with their values and ranges.

```{r}
options(digits = 3)

X <- read.csv("countries.csv", encoding = 'UTF-8',row.names = 1)

summary(X)

row.names(X)
```

### 2.2-(a)-(ii)

Plot a boxplot of all of the attributes. There are two outliers associated with the Pop boxplot. What do you think they represent?

Answer:

      That two outliers are the population of China and the population of India. That two outliers are real, not noise.

```{r}

X.standardized <- scale(X)

boxplot(X.standardized)
```

### 2.2-(b)-(i)

Print the summary of the PCA object. How many components explain at least 90% of the variance?

Answer:

      4 components.

```{r}

pca <- prcomp(X, scale.=T)
summary(pca)
```

### 2.2-(b)-(ii)

Draw a screeplot of the PCA object.

```{r}
screeplot(pca,type="l",main="Proportion of Variance in model")
```

### 2.2-(b)-(iii)

Based on the screeplot, how many principal components would you use for modeling?

Answer:

      I will use first 3 principal components for modeling. The first 3 principal components have effect for more than 80%.

### 2.2-(c)-(i)

Print the PCA components (the "rotation" field of the PCA object). Let's focus on PC1 and PC2.

Which attributes is PC1 positively correlated with, and which attributes is it negatively correlated with? Based on this, what is your expectation of PC1?

Answer:

      PC1 positively is correlated with GDP, Lifeexp, Oilcons and Tel attributes.
      PC1 negatively is correlated with HIV, Mil, Pop, Unempl attributes.

```{r}

pca$rotation

pca$rotation[,1:2]

```

### 2.2-(c)-(ii)

Which attributes is PC2 positively correlated with, and which attributes is it negatively correlated with? Based on this, what is your expectation of PC2?

Answer:

      PC2 positively is correlated with GDP, Lifeexp, Mil, Oilcons, Pop and Tel attributes.
      PC2 negatively is correlated with HIV, Unempl attributes.

### 2.2-(d)-(i)

Draw a biplot with the first and second components. Examine the rotated variables (the "x" field of the PCA object) for the first and second component for Brazil, UK, and Japan. Print these two columns out.

```{r}

biplot(pca)

print(pca$x[c("Brazil","UK","Japan"), 1:2])
```

### 2.2-(d)-(ii)

Using the information in (d)(i)(ii), provide reasons whether the values for PC1 and PC2 for Brazil, UK, and Japan make sense.

Answer:

      Brazil is a rapidly developing country. Compared with other countries, Brazil has more prominent problems in terms of HIV population ratio, military spending ratio, population size, and population unemployment rate.
      As a developed country, the UK is inferior to Japan in terms of GDP, life expectancy, oil consumption per capita, and fixed telephone numbers per capita.
      However, Japan does not perform well in terms of HIV population ratio and population unemployment rate.
